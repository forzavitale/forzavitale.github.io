---
layout: post
title: IMDb Top 250 Movies -- Data Analysis
---
![scatter](../images/moviescatterplot.png)

In this project, we examine data on the top 250-rated movies on IMDb.com to try and determine which factors are most helpful in predicting a movie's user rating.  Data are obtained via both the IMDBpie API (https://github.com/richardasaurus/imdb-pie) and scraping with BeautifulSoup.  


Since we are tasked with determining which factors most accurately predict user rating, let's take a look at the distribution of number of voters:

![histo](../images/movievotehisto.png)


The general shape of the histogram makes sense; we might expect the distribution of number of votes for ALL movies -- i.e., not just the top 250 -- to be normal.  Since we are only looking at the righthand slice of that, the mean of the total distribution should be on the left.  In the tail are blockbuster hits with millions of votes.

What kinds of ratings do we see in the top 250 movies?  Relatively high, clearly!  Here are some stats and a histogram to illustrate:

mean       8.315200

min        8.000000

median     8.300000

max        9.300000

![histo](../images/movieratinghisto.png)


What are the release years for the top 250 movies?  Perhaps unsurprisingly (when taking into account the present year and when the internet/IMDb came into full swing), a larger proportion of the movies at the top of the list were released sooner rather than later.  What size population is going to vote up a Charlie Chaplin movie in 2016?  A small one, possibly size 0!  With a bar space for each year, we can see that 1957, 1995, and 2015/16 (to date) had the most movies.

1957:  12 Angry Men, Paths of Glory, Witness for the Prosecution, The Bridge on the River Kwai, The Seventh Seal, Wild Strawberries, Nights of Cabiria, Throne of Blood

1995:  Se7en, The Usual Suspects, Braveheart, Toy Story, Heat, Casino, Before Sunrise, Twelve Monkeys, La Haine

![histo](../images/movieyearhisto.png)


The plot below displays very nicely an apparent positive correlation between number of votes and user rating of a movie in the Top 250.  Additionally, we see with the aid of color that older movies (colder/blue) tend to have fewer votes AND a lower rating than newer movies (warmer/red):

![jitter](../images/moviejitter.png)


What time of year are most movies released?  Well, of the top 250, looks like movies tended to come out in winter, summer, and around Halloween, maybe?  Low-density release months are April and September.

![bar](../images/moviemonthbar.png)


Now, let's plot number of votes against year of release.  We can glean a lot of information from the plot below!  Looks like all 'Approved' movies came out a while ago; that dummy variable will probably not be significant in a final model for predicting user rating.  Most 'Not Rated movies' came out a while ago, as well, except for a few every once in a while over more recent years.  As we will see from the following plot, these are all foreign (non-USA).  Most high-rated movies in recent years tend to be rated 'R', with some 'PG-13'.  'PG' and 'G' also have a fair amount of real estate in the top 250 but do not get quite as many votes as 'R' and 'PG-13'.  

![scatter](../images/moviescatterplot.png)


Here, we see that domestic films stopped getting MPAA's 'Not Rated'/'Unrated' certification a while ago, and the movies that still do get that every once in a while are all foreign.  We can also see that, while foreign films make up a fair proportion of the top 250 movies, they consistently have fewer voters than domestic movies.  This is possibly because IMDb is an American site.  As a recommendation, Netflix would do well to increase number of votes -- and thus, likely, traffic to the site -- by offering country-specific version of its site.

![scatter](../images/foreigndomesticNR.png)

In terms of model building, all of our tree-based methods (decision tree, extra trees, random forest, gradient boost, ADA boost, bagged decision trees) reported that number of votes, runtime, and year of release are the top three most important features in predicting user rating.

![graph](../images/movietreecurves.png)

The curves above display cumulative variance explained by the features included in the tree-based models referred to in the legend.  Decision tree performed fairly well this time but had a relatively high standard deviation.  ADA boost explained the most variance with the fewest features until the number of them reaches about thirty.  

Classifier tree-based models were used as well, performing ok but not amazingly; further work can be done integrating more features (e.g. leading actors, supporting actors, language of release, budget, opening weekend figures, etc.) to improve them.  
